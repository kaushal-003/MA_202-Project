Everyone in the computer theory community is familiar with the idea of randomization. It is not an exaggeration to state that randomization is one of the
most popular techniques for algorithm creation right now. The field of randomized algorithms has expanded significantly during the previous decade. At
this time, randomized algorithms progressed from being a tool in computational
number theory to finding broad use in a wide range of methods. Its expansion
has been fueled by two advantages of randomization: simplicity and speed. A
randomized algorithm is great many times, either the simplest or the quickest
algorithm accessible for various purposes.
We know very little about the input distribution in many cases. Even if we
have some information on the distribution, we may not be able to simulate it
computationally. As a result, we frequently use randomness and probabilistic
analysis as tools for designing and analyzing algorithms by making some of the
algorithmâ€™s behaviour random.
This paper discusses some important algorithms deliberately and how probability plays a crucial role in them. The algorithms which are discussed in this
paper are Min-Cut, Median, and the Miller-Rabin Primality Testing algorithms.
